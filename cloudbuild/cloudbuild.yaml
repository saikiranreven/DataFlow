steps:
  # 1. Terraform Infrastructure
  - id: 'terraform-init'
    name: 'hashicorp/terraform:1.5.7'
    args: ['init']
    dir: 'terraform'

  - id: 'terraform-apply'
    name: 'hashicorp/terraform:1.5.7'
    args: ['apply', '-auto-approve']
    dir: 'terraform'
    waitFor: ['terraform-init']

  # 2. Build Publisher Docker Image
  - id: 'build-publisher'
    name: 'gcr.io/cloud-builders/docker'
    args:
      - 'build'
      - '-t'
      - 'gcr.io/bct-project-465419/pubsub-publisher'
      - '-f'
      - 'publisher/Dockerfile'
      - 'publisher/'
    waitFor: ['terraform-apply']

  - id: 'push-publisher'
    name: 'gcr.io/cloud-builders/docker'
    args:
      - 'push'
      - 'gcr.io/bct-project-465419/pubsub-publisher'
    waitFor: ['build-publisher']

  # 3. Deploy Publisher to Cloud Run
  - id: 'deploy-publisher'
    name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    args:
      - 'gcloud'
      - 'run'
      - 'deploy'
      - 'pubsub-publisher'
      - '--image=gcr.io/bct-project-465419/pubsub-publisher'
      - '--platform=managed'
      - '--region=us-central1'
      - '--service-account=dataflow-ca@bct-project-465419.iam.gserviceaccount.com'
      - '--allow-unauthenticated'
      - '--memory=256Mi'
      - '--cpu=1'
    waitFor: ['push-publisher']

  # 4. Deploy Dataflow Job
  - id: 'deploy-dataflow'
    name: 'python:3.10'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        set -e
        pip install apache-beam[gcp]==2.54.0
        python dataflow/pipeline.py \
          --project=bct-project-465419 \
          --region=us-central1 \
          --runner=DataflowRunner \
          --input_topic=projects/bct-project-465419/topics/stream-topic \
          --output_table=bct-project-465419:streaming_dataset.user_events \
          --output_path=gs://bct-project-465419-raw-backup/raw/events \
          --temp_location=gs://bct-project-465419-raw-backup/temp \
          --staging_location=gs://bct-project-465419-raw-backup/staging

options:
  logging: CLOUD_LOGGING_ONLY
timeout: 1800s